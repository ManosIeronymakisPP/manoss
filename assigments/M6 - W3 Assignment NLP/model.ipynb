{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c77f683",
   "metadata": {},
   "source": [
    "# M6- W3 Assignment: NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16ade98",
   "metadata": {},
   "source": [
    "Natural language processing (NLP) is an important and rapidly developing part of machine learning. New powerful  models (the so-called transformer type) appear regularly and each new one outperforms the previous one in a fundamental NLP task, such as question-answering, name-entity recognition, etc. However, often simple, classical methods tend to work quite well and are a good first approach to solve many NLP problems.\n",
    "\n",
    "In this assignment, you will work with a famous data set for sentiment analysis, namely the Amazon reviews data set. One place where the data can be found is here: https://www.kaggle.com/bittlingmayer/amazonreviews (Links to an external site.)Links to an external site.\n",
    "\n",
    "Download and import the training and testing data sets. It’s not in a usual .csv format, so the below code can help you transform it to a pandas data frame.\n",
    "     import bz2\n",
    "\n",
    "      train_file = bz2.BZ2File(\"train.ft.txt.bz2\")\n",
    "\n",
    "      # Load and decode\n",
    "\n",
    "      lines = [x.decode('utf-8') for x in train_file.readlines()]\n",
    "\n",
    "      # Split in two: sentiment and review\n",
    "\n",
    "      score_review_list = [l.strip('__label__').split(' ', 1) for l in lines]\n",
    "\n",
    "      df = pd.DataFrame(score_review_list, columns = ['score', 'review'] )\n",
    "\n",
    "Bonus points for extracting reviews and labels using regular expressions and named groups.\n",
    "Create a new feature, called ‘n_tokens’ that counts how many tokens(words) there are in a review. In other words, a feature for the length of a review.\n",
    "Create a new feature, called ‘language’, which detects what is the language of each review. So this feature will have a different value for each row (review) of the data.\n",
    "Transform each review into a numeric vector of tokens using a bag-of-words. Use can use the CountVectorizer module from sklearn but limit the maximum number of features to be 1000 to avoid memory issues (you can decrease it further if you still have memory issues). Explore the other parameters of the function as well.\n",
    "Using the fitted and transformed vector and the above created features, train a model that predicts the sentiment of a review. Note that this will be a classification problem. Evaluate your model and motivate your choice of a performance metric.(Hint: the feature for language is of type ‘object’, you may want to transform it to binary, such that it is 1 if the language is in English, 0 otherwise)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3c841c",
   "metadata": {},
   "source": [
    "## Step 1: Download and Import the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe848a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\ManosIeronymakisProb\\\\OneDrive - Probability\\\\Bureaublad\\\\ELU\\M6- W3 Assignment NLP\")\n",
    "\n",
    "train_file = bz2.BZ2File(\"train.ft.txt.bz2\")\n",
    "\n",
    "## Load and decode\n",
    "lines = [x.decode('utf-8') for x in train_file.readlines()]\n",
    "\n",
    "# Split into sentiment and review\n",
    "score_review_list = [l.strip('__label__').split(' ', 1) for l in lines]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(score_review_list, columns=['score', 'review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8944c314",
   "metadata": {},
   "source": [
    "## Step 2: Extract Reviews and Labels using Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f68f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "score_review_list = [re.match(r\"__label__(\\d+) (.*)\", l).groups() for l in lines]\n",
    "\n",
    "df = pd.DataFrame(score_review_list, columns=['score', 'review'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a678b7",
   "metadata": {},
   "source": [
    "## Step 3: Create the 'n_tokens' Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f6a1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['n_tokens'] = df['review'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd439ae",
   "metadata": {},
   "source": [
    "## Step 4: Create the 'language' Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c779b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "import numpy as np\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        if len(text.strip()) == 0:\n",
    "            return np.nan  # Return NaN for empty reviews\n",
    "        else:\n",
    "            return detect(text)\n",
    "    except:\n",
    "        return np.nan  # Return NaN if language detection fails\n",
    "\n",
    "df['language'] = df['review'].apply(detect_language)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4915e0",
   "metadata": {},
   "source": [
    "## Step 5: Transform Reviews into Numeric Vectors using Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3cb12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(df['review']).toarray()\n",
    "\n",
    "# Create a new DataFrame with the transformed vectors\n",
    "df_transformed = pd.DataFrame(X, columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Concatenate the transformed DataFrame with the original DataFrame\n",
    "df = pd.concat([df, df_transformed], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb333e2",
   "metadata": {},
   "source": [
    "## Step 6: Prepare the Data for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59873275",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['language'] = df['language'].apply(lambda x: 1 if x == 'en' else 0)\n",
    "\n",
    "X = df.drop(['score', 'review'], axis=1)\n",
    "y = df['score']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6b24cf",
   "metadata": {},
   "source": [
    "## Step 7: Train a Model for Sentiment Classification and Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85235a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
