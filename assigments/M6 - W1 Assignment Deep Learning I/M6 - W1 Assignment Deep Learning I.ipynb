{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "526780cd",
   "metadata": {},
   "source": [
    "# M6 - W1 Assignment: Deep Learning I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc981043",
   "metadata": {},
   "source": [
    "The MNIST dataset is a very famous dataset used to test and benchmark new deep learning architectures and models. It contains images of handwritten digits (from 0 to 9). It consists of a training and test sets of features and labels.\n",
    "\n",
    "- Your first task is to use PyTorch and develop a model that correctly detects a handwritten digit. Though you can technically solve that task with any type of a supervised model, please use Feed-Forward neural networks. You are free to experiment with the architecture of the model. Extra points will be awarded if you define your own model class rather than a Sequential model.\n",
    "- Please evaluate your model properly and interpret its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05a0ed88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.1749\n",
      "Epoch [2/10], Loss: 0.0844\n",
      "Epoch [3/10], Loss: 0.1225\n",
      "Epoch [4/10], Loss: 0.0965\n",
      "Epoch [5/10], Loss: 0.0171\n",
      "Epoch [6/10], Loss: 0.0509\n",
      "Epoch [7/10], Loss: 0.1134\n",
      "Epoch [8/10], Loss: 0.0082\n",
      "Epoch [9/10], Loss: 0.0078\n",
      "Epoch [10/10], Loss: 0.1302\n",
      "Precision: 0.9703\n",
      "Recall: 0.9701\n",
      "F1-score: 0.9701\n",
      "Loss: 0.1302\n",
      "Example predictions:\n",
      "True: 7, Predicted: 7\n",
      "True: 2, Predicted: 2\n",
      "True: 1, Predicted: 1\n",
      "True: 0, Predicted: 0\n",
      "True: 4, Predicted: 4\n",
      "True: 1, Predicted: 1\n",
      "True: 4, Predicted: 4\n",
      "True: 9, Predicted: 9\n",
      "True: 5, Predicted: 5\n",
      "True: 9, Predicted: 9\n",
      "True: 0, Predicted: 0\n",
      "True: 6, Predicted: 6\n",
      "True: 9, Predicted: 9\n",
      "True: 0, Predicted: 0\n",
      "True: 1, Predicted: 1\n",
      "True: 5, Predicted: 5\n",
      "True: 9, Predicted: 9\n",
      "True: 7, Predicted: 7\n",
      "True: 3, Predicted: 8\n",
      "True: 4, Predicted: 4\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define transformations to apply to the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset using pandas\n",
    "os.chdir(\"C:\\\\Users\\\\ManosIeronymakisProb\\\\OneDrive - Probability\\\\Bureaublad\\\\ELU\\\\M6 - W1 Assignment Deep Learning I\")\n",
    "\n",
    "train_filepath = \"mnist_train.csv\"\n",
    "test_filepath = \"mnist_test.csv\"\n",
    "\n",
    "train_data = pd.read_csv(train_filepath)\n",
    "test_data = pd.read_csv(test_filepath)\n",
    "\n",
    "# Separate features and labels\n",
    "train_labels = train_data['label']\n",
    "train_features = train_data.drop('label', axis=1)\n",
    "\n",
    "test_labels = test_data['label']\n",
    "test_features = test_data.drop('label', axis=1)\n",
    "\n",
    "# Convert the data to tensors\n",
    "train_features_tensor = torch.tensor(train_features.values, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels.values, dtype=torch.long)\n",
    "test_features_tensor = torch.tensor(test_features.values, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels.values, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_features_tensor, train_labels_tensor)\n",
    "test_dataset = TensorDataset(test_features_tensor, test_labels_tensor)\n",
    "\n",
    "# Define data loaders\n",
    "batch_size = 128\n",
    "num_workers = 2 if device.type == \"cuda\" else 0  # Adjust the number of workers based on GPU availability\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# Define the model class\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = NeuralNet().to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        # Move data to GPU if available\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        # Move data to GPU if available\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-score: {f1_score:.4f}')\n",
    "print(f'Loss: {loss.item():.4f}')\n",
    "\n",
    "# Print true label and predicted label for 20 examples\n",
    "print(\"Example predictions:\")\n",
    "for i in range(20):\n",
    "    print(f'True: {true_labels[i]}, Predicted: {predicted_labels[i]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3990c8e9",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e4860",
   "metadata": {},
   "source": [
    "Epoch [1/10], Loss: 0.1749: This means that during the first epoch of training, the average loss on the training set was 0.1749.\n",
    "\n",
    "Epoch [2/10], Loss: 0.0844: In the second epoch, the average loss on the training set decreased to 0.0844, indicating that the model is improving in its ability to minimize the difference between predicted and actual labels.\n",
    "\n",
    "Epoch [3/10], Loss: 0.1225: The loss slightly increased in the third epoch to 0.1225, which could indicate a slight overfitting or noise in the training data.\n",
    "\n",
    "Epoch [4/10], Loss: 0.0965: The loss decreased again in the fourth epoch to 0.0965, suggesting that the model is learning to generalize better.\n",
    "\n",
    "Epoch [5/10], Loss: 0.0171: The loss significantly decreased to 0.0171 in the fifth epoch, indicating a significant improvement in the model's performance.\n",
    "\n",
    "Epoch [6/10], Loss: 0.0509: The loss slightly increased in the sixth epoch to 0.0509, but it still remains relatively low.\n",
    "\n",
    "Epoch [7/10], Loss: 0.1134: The loss increased again in the seventh epoch to 0.1134, which might indicate some fluctuations in the training process.\n",
    "\n",
    "Epoch [8/10], Loss: 0.0082: The loss decreased sharply to 0.0082 in the eighth epoch, suggesting that the model is converging well.\n",
    "\n",
    "Epoch [9/10], Loss: 0.0078: The loss remained low in the ninth epoch at 0.0078.\n",
    "\n",
    "Epoch [10/10], Loss: 0.1302: The loss increased in the tenth and final epoch to 0.1302, but it is still relatively low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff0499d",
   "metadata": {},
   "source": [
    "In the context of handwritten digit recognition, it is important to evaluate the performance of the model using appropriate metrics. In this project, we considered precision, recall, F1-score, and loss as the key metrics. Loss serves as a fundamental metric during training, guiding the model towards minimizing the discrepancy between predicted and true labels. Precision measures the model's ability to correctly identify a specific digit, focusing on minimizing false positives. Recall, on the other hand, evaluates the model's ability to capture all instances of a particular digit, aiming to minimize false negatives. The F1-score provides a balanced measure, taking into account both precision and recall, and is particularly useful when dealing with imbalanced datasets or uneven class distributions. By considering these metrics, we gain valuable insights into the model's performance and can make informed decisions based on the specific requirements of the problem at hand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3e640b",
   "metadata": {},
   "source": [
    "The model achieved an accuracy of approximately 97% on the test set. Precision, Recall, and F1-score were calculated and found to be 0.9703, 0.9701, and 0.9701 respectively. These metrics indicate that the model performs well in classifying the handwritten digits. The loss value at the end of training was 0.1302. The printed examples show the true label and predicted label for 20 random samples from the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
