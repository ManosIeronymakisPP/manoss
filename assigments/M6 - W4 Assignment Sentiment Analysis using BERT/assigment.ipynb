{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26f7362a",
   "metadata": {},
   "source": [
    "# M6 - W4 Assignment: Sentiment Analysis using BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d9444d",
   "metadata": {},
   "source": [
    "The objective of this exercise is to evaluate the performance of a Transformers-based model using the Hugging Face library on the IMDb movie review dataset for sentiment classification.\n",
    "\n",
    "Dataset: The IMDb dataset consists of 50,000 movie reviews, with an equal number of positive and negative reviews. It is commonly used for sentiment analysis tasks. You can download the dataset from the following link: http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gzLinks to an external site.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Step 1: Prepare the Data\n",
    "\n",
    "Extract the downloaded dataset and load the training and testing data into appropriate data structures.\n",
    "Preprocess the data as required, which may include tokenization, padding, or any other necessary steps.\n",
    "Step 2: Select and Load a Pre-trained Model\n",
    "\n",
    "Choose a pre-trained Transformers-based model suitable for text classification, such as BERT or RoBERTa.\n",
    "Load the pre-trained model using the Hugging Face library.\n",
    "Step 3: Fine-tune the Model\n",
    "\n",
    "Define the appropriate architecture and layers for fine-tuning the pre-trained model.\n",
    "Fine-tune the model on the IMDb dataset by training it on the training data.\n",
    "Adjust the hyperparameters as needed, including the learning rate and batch size. Do a CV for various parameters of the model.\n",
    "Step 4: Evaluate the Model\n",
    "\n",
    "Use the fine-tuned model to make predictions on the testing data.\n",
    "Calculate the accuracy, precision, recall, and F1-score of the model's predictions.\n",
    "Use appropriate evaluation metrics based on the nature of the classification task.\n",
    "Additional Notes:\n",
    "\n",
    "You can refer to the Hugging Face documentation for guidance on using their library and working with Transformers models: https://huggingface.co/transformers/Links to an external site.\n",
    "Make sure to handle any necessary data preprocessing, such as cleaning or normalizing the text, before feeding it into the model.\n",
    "It is recommended to use a GPU if available to speed up the training process.\n",
    "Feel free to explore and experiment beyond the provided instructions to gain a deeper understanding of Transformers-based models and their applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb46870a",
   "metadata": {},
   "source": [
    "## Step 1: Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c23090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Path to the downloaded gz file\n",
    "gz_file_path = \"C:/Users/ManosIeronymakisProb/OneDrive - Probability/Bureaublad/ELU/M6 - W4 Assignment Sentiment Analysis using BERT/aclImdb_v1.tar.gz\"\n",
    "\n",
    "# Directory to store the extracted dataset\n",
    "dataset_dir = \"imdb_dataset\"\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# Extract the gz file to the dataset directory\n",
    "with tarfile.open(gz_file_path, \"r:gz\") as tar:\n",
    "    tar.extractall(dataset_dir)\n",
    "\n",
    "# Function to load reviews and labels from a directory\n",
    "def load_reviews(directory):\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for label in ['pos', 'neg']:\n",
    "        path = os.path.join(directory, label)\n",
    "        for filename in os.listdir(path):\n",
    "            with open(os.path.join(path, filename), 'r', encoding='utf-8') as file:\n",
    "                review = file.read()\n",
    "            reviews.append(review)\n",
    "            labels.append(1 if label == 'pos' else 0)\n",
    "    return reviews, labels\n",
    "\n",
    "# Directory paths for the training and testing sets\n",
    "train_dir = r\"C:\\Users\\ManosIeronymakisProb\\OneDrive - Probability\\Bureaublad\\ELU\\M6 - W4 Assignment Sentiment Analysis using BERT\\imdb_dataset\\aclImdb\\train\"\n",
    "test_dir = r\"C:\\Users\\ManosIeronymakisProb\\OneDrive - Probability\\Bureaublad\\ELU\\M6 - W4 Assignment Sentiment Analysis using BERT\\imdb_dataset\\aclImdb\\test\"\n",
    "\n",
    "# Load the reviews and labels for training set\n",
    "train_reviews, train_labels = load_reviews(train_dir)\n",
    "\n",
    "# Load the reviews and labels for testing set\n",
    "test_reviews, test_labels = load_reviews(test_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e810d0",
   "metadata": {},
   "source": [
    "## Step 2: Select and Load a Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e500297b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model name\n",
    "model_name = \"bert-base-uncased\"  # Example: BERT-base\n",
    "\n",
    "# Load the tokenizer and model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abeda57",
   "metadata": {},
   "source": [
    "## Step 3 & 4: Fine-tune the Model and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2fa7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for fine-tuning\n",
    "train_encodings = tokenizer(train_reviews, truncation=True, padding=True, return_tensors='pt')\n",
    "train_inputs = train_encodings['input_ids']\n",
    "train_attention_mask = train_encodings['attention_mask']\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_dataset = TensorDataset(train_inputs, train_attention_mask, train_labels)\n",
    "\n",
    "# Define the hyperparameters and perform cross-validation\n",
    "learning_rates = [2e-5, 3e-5, 5e-5]\n",
    "batch_sizes = [16, 32, 64]\n",
    "num_epochs = 3\n",
    "num_folds = 5\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_model = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        optimizer = AdamW(model.parameters(), lr=lr)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        scheduler = get_scheduler(\"linear\", optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * num_epochs)\n",
    "\n",
    "        fold_accuracy = 0.0\n",
    "        kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "        \n",
    "        for fold, (train_indices, val_indices) in enumerate(kfold.split(train_dataset)):\n",
    "            train_fold_dataset = TensorDataset(train_dataset[train_indices][0], train_dataset[train_indices][1], train_dataset[train_indices][2])\n",
    "            val_fold_dataset = TensorDataset(train_dataset[val_indices][0], train_dataset[val_indices][1], train_dataset[val_indices][2])\n",
    "\n",
    "            train_fold_dataloader = DataLoader(train_fold_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_dataloader = DataLoader(val_fold_dataset, batch_size=batch_size)\n",
    "\n",
    "            model.train()\n",
    "            for epoch in range(num_epochs):\n",
    "                for batch in train_fold_dataloader:\n",
    "                    optimizer.zero_grad()\n",
    "                    input_ids, attention_mask, labels = batch\n",
    "                    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "                    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                    loss = outputs.loss\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_preds = []\n",
    "                for batch in val_dataloader:\n",
    "                    input_ids, attention_mask, labels = batch\n",
    "                    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "                    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                    logits = outputs.logits\n",
    "                    preds = torch.argmax(logits, dim=1)\n",
    "                    val_preds.extend(preds.tolist())\n",
    "\n",
    "                val_labels = train_dataset[val_indices][2]\n",
    "                accuracy = accuracy_score(val_labels, val_preds)\n",
    "                fold_accuracy += accuracy\n",
    "\n",
    "        average_accuracy = fold_accuracy / num_folds\n",
    "        if average_accuracy > best_accuracy:\n",
    "            best_accuracy = average_accuracy\n",
    "            best_model = model.state_dict()\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(best_model)\n",
    "\n",
    "# Prepare the test data\n",
    "test_encodings = tokenizer(test_reviews, truncation=True, padding=True, return_tensors='pt')\n",
    "test_inputs = test_encodings['input_ids']\n",
    "test_attention_mask = test_encodings['attention_mask']\n",
    "test_labels = torch.tensor(test_labels)\n",
    "test_dataset = TensorDataset(test_inputs, test_attention_mask, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_preds = []\n",
    "    for batch in test_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        test_preds.extend(preds.tolist())\n",
    "\n",
    "# Convert the predictions to numpy arrays\n",
    "test_preds = torch.tensor(test_preds).cpu().numpy()\n",
    "test_labels = test_labels.cpu().numpy()\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(test_labels, test_preds)\n",
    "precision = precision_score(test_labels, test_preds)\n",
    "recall = recall_score(test_labels, test_preds)\n",
    "f1 = f1_score(test_labels, test_preds)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
